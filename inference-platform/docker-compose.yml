version: "3.9"

services:
  # --------------------
  # Text2SQL - Snowflake Arctic R1 7B (GPU0)
  # --------------------
  text2sql:
    image: vllm/vllm-openai:latest
    container_name: vllm-text2sql
    restart: unless-stopped
    ipc: host
    environment:
      - HF_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ${HF_HOME:-./hf-cache}:/root/.cache/huggingface
    expose:
      - "8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      --model Snowflake/Arctic-Text2SQL-R1-7B
      --host 0.0.0.0
      --port 8000
      --api-key ${BACKEND_API_KEY}
      --max-model-len 8192
      --gpu-memory-utilization 0.95
      --dtype auto

  # --------------------
  # Chat - Qwen 2.5 7B (GPU1)
  # --------------------
  chat:
    image: vllm/vllm-openai:latest
    container_name: vllm-chat
    restart: unless-stopped
    ipc: host
    environment:
      - HF_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ${HF_HOME:-./hf-cache}:/root/.cache/huggingface
    expose:
      - "8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --host 0.0.0.0
      --port 8000
      --api-key ${BACKEND_API_KEY}
      --max-model-len 8192
      --gpu-memory-utilization 0.95
      --dtype auto

  # --------------------
  # Gateway (CPU) - Enterprise features
  # --------------------
  gateway:
    build: ./gateway
    container_name: inference-gateway
    restart: unless-stopped
    depends_on:
      - chat
      - text2sql
    environment:
      - GATEWAY_API_KEY=${GATEWAY_API_KEY}
      - BACKEND_API_KEY=${BACKEND_API_KEY}
      - CHAT_BACKENDS=${CHAT_BACKENDS}
      - TEXT2SQL_BACKEND=${TEXT2SQL_BACKEND}
      - MAX_RPS_PER_IP=${MAX_RPS_PER_IP}
      - RPS_WINDOW_SECS=${RPS_WINDOW_SECS}
      - RPS_BURST=${RPS_BURST}
      - MAX_INFLIGHT_PER_IP=${MAX_INFLIGHT_PER_IP}
      - QUEUE_TIMEOUT_SECS=${QUEUE_TIMEOUT_SECS}
      - MAX_REQUEST_SECS=${MAX_REQUEST_SECS}
      - STREAM_IDLE_TIMEOUT_SECS=${STREAM_IDLE_TIMEOUT_SECS}
      - ORG_DAILY_TOKEN_LIMIT=${ORG_DAILY_TOKEN_LIMIT}
      - ORG_DAILY_REQUEST_LIMIT=${ORG_DAILY_REQUEST_LIMIT}
      - ORG_MONTHLY_TOKEN_LIMIT=${ORG_MONTHLY_TOKEN_LIMIT}
      - CACHE_TTL_SECS=${CACHE_TTL_SECS}
      - CACHE_MAX_SIZE=${CACHE_MAX_SIZE}
      - CIRCUIT_FAILURE_THRESHOLD=${CIRCUIT_FAILURE_THRESHOLD}
      - CIRCUIT_RECOVERY_TIMEOUT=${CIRCUIT_RECOVERY_TIMEOUT}
      - HEALTH_CHECK_INTERVAL_SECS=${HEALTH_CHECK_INTERVAL_SECS}
      - HEALTH_CHECK_TIMEOUT_SECS=${HEALTH_CHECK_TIMEOUT_SECS}
      - LOG_LEVEL=${LOG_LEVEL}
      - ENABLE_PII_REDACTION=${ENABLE_PII_REDACTION}
      - GATEWAY_WORKERS=${GATEWAY_WORKERS}
    expose:
      - "9000"

  # --------------------
  # Nginx TLS (public)
  # --------------------
  nginx:
    image: nginx:1.27-alpine
    container_name: inference-nginx
    restart: unless-stopped
    depends_on:
      - gateway
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ../ssl_certificate:/etc/nginx/ssl:ro

networks:
  default:
    name: inference-network